# -*- coding: utf-8 -*-
"""Web Scraping

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aBwunO_uJydNHFPnKZkTiXg-DFf5wPv8
"""

#### Ejemplo de WebScraping con Python
## Obtener Ibex35 bolsa de Madrid


# import libraries
import requests
from bs4 import BeautifulSoup
import csv
from datetime import datetime

# indicar la ruta
url_page = 'http://www.bolsamadrid.es/esp/aspx/Indices/Resumen.aspx'

# tarda 480 milisegundos
page = requests.get(url_page).text 
soup = BeautifulSoup(page, "lxml")

# Obtenemos la tabla por un ID específico
tabla = soup.find('table', attrs={'id': 'ctl00_Contenido_tblÍndices'})
tabla

name=""
price=""
nroFila=0
for fila in tabla.find_all("tr"):
    if nroFila==1:
        nroCelda=0
        for celda in fila.find_all('td'):
            if nroCelda==0:
                name=celda.text
                print("Indice:", name)
            if nroCelda==2:
                price=celda.text
                print("Valor:", price)
            nroCelda=nroCelda+1
    nroFila=nroFila+1

# Abrimos el csv con append para que pueda agregar contenidos al final del archivo
with open('bolsa_ibex35.csv', 'a') as csv_file:
    writer = csv.writer(csv_file)
    writer.writerow([name, price, datetime.now()])


### Ejemplo Liga BBVA - España - Primera -  desde marcadores.com

url_page = 'https://www.marcadores.com/futbol/espana/liga-bbva/?competitionRoundId=486942' # jornada 20

# tarda 1500 milisegundos
page = requests.get(url_page).text 
soup = BeautifulSoup(page, "lxml")

# Obtenemos la tabla por un ID específico
tabla = soup.find('table', attrs={'class': 'matches'})
tabla

data = []
equipo1=""
equipo2=""
resultado=""
nroFila=0
for fila in tabla.find_all("tr"):
    if nroFila>0:
        nroCelda=0
        capturar=False
        for celda in fila.find_all('td'):
            if nroCelda==1 and celda.text=='Fin.':
                capturar=True
            if capturar and nroCelda==2:
                equipo1=celda.text
            if capturar and nroCelda==4:
                equipo2=celda.text
            if capturar and nroCelda==5:
                resultado=celda.text
                print("Partido:", equipo1,'vs',equipo2,resultado)
                data.append((equipo1,equipo2,resultado))
            nroCelda=nroCelda+1
    nroFila=nroFila+1

# Abrimos el csv con append para que pueda agregar contenidos al final del archivo
with open('partidos_liga_primera.csv', 'a') as csv_file:
    writer = csv.writer(csv_file)
    for equipo1, equipo2,resultado in data:
        writer.writerow([equipo1, equipo2, resultado,datetime.now()])

"""# Otros ejemplos de WebScaping"""

#supongamos tenemos el siguiente HTML
pagina_web = "<html>" \
            + "<head></head>" \
            + "<body>" \
                + "<div class='contenedor'>" \
                    + "<div id='123' name='bloque_bienvenida' class='verde'>" \
                        + "Bienvenido a mi web" \
                    + "</div>" \
                + "</div>" \
            + "</body>" \
            + "</html>"

soup = BeautifulSoup(pagina_web, "lxml")

#Obtener por ID:
elTexto = soup.find('div', attrs={'id': '123'}).getText()
print(elTexto)

#Obtener por Clase CSS:
elTexto = soup.find('div', attrs={'class': 'verde'}).getText()
print(elTexto)

#Obtener dentro de otra etiqueta anidado:
elTexto = next(soup.div.children).getText() #con next obtiene primer "hijo"
print(elTexto)

"""## Obtener items de un listado"""

#supongamos tenemos el siguiente HTML
pagina_web = "<html>" \
    + "<head></head>" \
    + "<body>" \
        + "<div class='contenedor'>" \
            + "<ul>" \
                + "<li>Perro</li>" \
                + "<li>Gato</li>" \
                + "<li>Tortuga</li>" \
            + "</ul>" \
        + "</div>" \
    + "</body>" \
    + "</html>"

soup = BeautifulSoup(pagina_web, "lxml")

for child in soup.ul.children:
    print(child.getText())

items = soup.find_all('li')
for item in items:
    print(item.getText())

"""## Obtener Enlaces"""

#supongamos tenemos el siguiente HTML
pagina_web = "<html>" \
    + "<head></head>" \
    + "<body>" \
        + "<div class='contenedor'>" \
            + "<ul>" \
                + "<li><a href='http://www.google.com'>Google</a></li>" \
                + "<li><a href='http://www.yahoo.com'>Yahoo</a></li>" \
                + "<li><a href='http://www.bing.com'>Bing</a></li>" \
            + "</ul>" \
        + "</div>" \
    + "</body>" \
    + "</html>"

soup = BeautifulSoup(pagina_web, "lxml")

items = soup.find_all('a')
for item in items:
    print(item['href'])

"""## Ejemplo completo Extraer enlaces"""

url_page = 'https://www.lifeder.com/cientificos-famosos/'
page = requests.get(url_page).text 
soup = BeautifulSoup(page, "lxml")
contenido = soup.find('div', attrs={'class': 'td-post-content'})
items = contenido.find_all('a')
for item in items:
    print(item['href'])

"""El artículo completo en www.aprendemachinelearning.com

# Ejemplo de página del clíma
"""

import requests
from bs4 import BeautifulSoup

page = requests.get("http://forecast.weather.gov/MapClick.php?lat=37.7772&lon=-122.4168")
soup = BeautifulSoup(page.content, 'lxml')
seven_day = soup.find(id="seven-day-forecast")
forecast_items = seven_day.find_all(class_="tombstone-container")
tonight = forecast_items[0]
print(tonight.prettify())

period = tonight.find(class_="period-name").get_text()
short_desc = tonight.find(class_="short-desc").get_text()
temp = tonight.find(class_="temp").get_text()
print(period)
print(short_desc)
print(temp)

img = tonight.find("img")
desc = img['title']
print(desc)

period_tags = seven_day.select(".tombstone-container .period-name")
periods = [pt.get_text() for pt in period_tags]
periods

short_descs = [sd.get_text() for sd in seven_day.select(".tombstone-container .short-desc")]
temps = [t.get_text() for t in seven_day.select(".tombstone-container .temp")]
descs = [d["title"] for d in seven_day.select(".tombstone-container img")]
print(short_descs)
print(temps)
print(descs)

import pandas as pd
weather = pd.DataFrame({
    "period": periods,
    "short_desc": short_descs,
    "temp": temps,
    "desc":descs
})
weather

temp_nums = weather["temp"].str.extract("(?P<temp_num>\d+)", expand=False)
weather["temp_num"] = temp_nums.astype('int')
temp_nums

weather["temp_num"].mean()

is_night = weather["temp"].str.contains("Low")
weather["is_night"] = is_night
is_night

weather[is_night]

### Ejemplo de Linio - Lavadoras

import requests
from bs4 import BeautifulSoup
import numpy as np
import pandas as pd

page = requests.get("https://www.linio.com.co/search?scroll=&q=lavadoras+semiautomatica")
soup = BeautifulSoup(page.content, 'lxml')
lavado = soup.find(id="catalogue-product-container")

desc = '' # Descrición de producto
brand = '' # Marca
pvp = '' # Precio de venta al público
discount = '' # Descuento en %
offer = '' # Precio con descuento $
offer_ad = '' # Descuento adicional con otros medios de pago
data = []
for ref in lavado.find_all(class_="catalogue-product row"):
  # Descripción del producto
  img = ref.find("img")
  desc = img['alt']
  # Marca
  item = ref.find_all('meta')[4]
  brand = item['content']
  # PVP
  if ref.find_all(class_ = "original-price") == []:
    pvp = '$0'
  else:
    pvp = ref.find_all(class_ = "original-price")[0].getText()
  # Descuento en %
  if ref.find_all(class_ = "discount") == []:
    discount = '0%'
  else:
    discount = ref.find_all(class_ = "discount")[0].getText()
  # Descuento en $
  if ref.find_all(class_ = "price-main-md") == []:
    offer = '$0'
  else:
    offer =  ref.find_all(class_ = "price-main-md")[0].getText()
  # Descuento con otros medios de pago
  if ref.find_all(class_ = "price-promotional") == []:
    offer_ad = '$0'
  else:
    offer_ad =  ref.find_all(class_ = "price-promotional")[0].getText()
  data.append((desc, brand, pvp, discount, 
               offer.rstrip('\n').replace('\n', ''), offer_ad))

dataset = pd.DataFrame(data, columns= ['Descripcion', 'Marca', 'PVP', 
                                       'Descuento', 'PrecioConDescuento', 
                                       'OfertaConTarjeta'])